# Ridge Regression
The first version of regression was invented more than a hundred years ago.  
While the core concept of the early version remains almost the same, the old method was susceptible to overfitting and multicollinearity as more predictors are added onto an equation.  
Therefore, some other methods were invented to put a check on a variance from growing too much and to find the optimal point which is often times called "sweet spot" where total error is the minimum.  
The Ridge Regression and Lasso Regression) are well known methods to the job called Regularization) or Regression Shrinkage Method).
([brouder explanation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation))

## Code
[`python3 sample.py`](./sample.py)
<p align="center">
  <img src="https://pbs.twimg.com/media/ERf2EKCWAAcLBCH.jpg" width="750px">
</p>
<p align="center">
  <img src="https://miro.medium.com/max/411/1*Yu3iBnyHL7skNiidHxmEGQ.png" width="750">
</p>
<p align="center">
  <img src="https://www.tutorialexample.com/wp-content/uploads/2019/05/ridge-regression.png" width="750">
</p>

# Resources
- https://bookdown.org/tpinto_home/Regularisation/ridge-regression.html
- https://bradleyboehmke.github.io/HOML/regularized-regression.html
- https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db
- https://towardsdatascience.com/how-to-code-ridge-regression-from-scratch-4b3176e5837c
- https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/